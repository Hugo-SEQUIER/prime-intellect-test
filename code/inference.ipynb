{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"llama_model\" # Depends of the directory storage\n",
    "device='cuda'\n",
    "dtype=torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=dtype, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"cais/mmlu\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Select the choice from the given 'Choices'.\n",
    "It's Very Important to have an output containing THE CHOICE !!!!\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Choices:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    for i in range(total):\n",
    "        question = dataset[i][\"question\"]\n",
    "        choices = dataset[i][\"choices\"]\n",
    "        input_ids = tokenizer([\n",
    "                alpaca_prompt.format(\n",
    "                    question, # instruction\n",
    "                    choices,\n",
    "                    \"\", # output - leave this blank for generation!\n",
    "                )\n",
    "            ], return_tensors=\"pt\").to(model.device)\n",
    "        output_ids = model.generate(**input_ids, max_length=2048, num_return_sequences=1)\n",
    "        # Decode and print the output\n",
    "        output_text = tokenizer.batch_decode(output_ids)[0].strip()\n",
    "        output_text = output_text.split(\"Response:\\n\")[1].split(\"<|\")[0].replace(\"'\",\"\").replace('.','').split(' ')[-1] # Ensure output is cleaned up of any extra whitespace\n",
    "        if output_text == int(dataset[i][\"answer\"]) or output_text == dataset[i]['choices'][int(dataset[i]['answer'])]:\n",
    "          correct += 1 \n",
    "                \n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "for i in range(5):\n",
    "    accuracy = evaluate_model(model, ds['test'])\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "print(sum(accuracy_list) / len(accuracy_list))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
